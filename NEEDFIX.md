# RLtools Plugin Implementation - Items Needing Attention

This document outlines discrepancies and incomplete implementations found when comparing the `TODOs.md` (Phases 1-3) with the current source code.

## Phase 1: RLtools Integration and Basic Setup

1.  **Item 1.2.1: Path to `rl_tools` Library**
    *   **Issue:** `TODOs.md` specifies the path as `UERLTools/Source/ThirdParty/rl_tools_lib/`.
    *   **Actual:** The actual path is `UERLTools/Source/ThirdParty/rl_tools/`. The `UERLTools.Build.cs` file uses the correct actual path.
    *   **Recommendation:** Update `TODOs.md` (Task 1.2.1) to reflect the correct directory name (`rl_tools`) for consistency.

## Phase 2: C++ Abstraction Layer for RLtools in UE

1.  **Item 2.1.2: `RLTOOLS_NAMESPACE_WRAPPER`**
    *   **Issue:** The suggested `RLTOOLS_NAMESPACE_WRAPPER` (from `doc.md`) is not used in the current codebase. `rl_tools` types are accessed directly via `rlt::`.
    *   **Recommendation:** Decide if this wrapper is still desired. If not, this point can be removed from `TODOs.md` or marked as by design.

2.  **Item 2.1.3 & 3.5.1: Data Exchange/Conversions (`TArray<float>` to/from `rl_tools::Matrix`)**
    *   **Issue:** Critical C++ logic for converting between UE's `TArray<float>` (used for observations/actions in Blueprint interfaces) and `rl_tools::Matrix` types (required for internal `rl_tools` operations) is missing or placeholder in `URLAgentManager.cpp`.
    *   **Impact:** The agent cannot currently process UE observations with `rl_tools` policies or feed UE actions generated by `rl_tools` back to the environment correctly, nor can it correctly populate the replay buffer or perform training updates with `rl_tools`.
    *   **Recommendation:** Implement robust conversion functions within `URLAgentManager.cpp`.

3.  **Item 2.2.2: `rl_tools` Custom Environment C++ API**
    *   **Issue:** `URLEnvironmentComponent` exposes environment functionalities to Blueprints. However, the direct C++ interface (e.g., specific `rlt::step`, `rlt::observe` functions or an adapter class) that `rl_tools` algorithms expect to interact with a custom environment is not implemented. 
    *   **Impact:** `URLAgentManager` cannot directly pass `URLEnvironmentComponent` to `rl_tools` algorithms like `OffPolicyRunner` which expect a compatible C++ environment interface.
    *   **Recommendation:** Implement an adapter class within the plugin that wraps `URLEnvironmentComponent` and exposes the required `rlt::` environment functions, calling the corresponding `URLEnvironmentComponent` methods internally.

4.  **Item 2.3.2 & 2.3.3: `URLAgentManager` - `rl_tools` Algorithm Setup & Initialization**
    *   **Issue:** The C++ implementation in `URLAgentManager.cpp` for initializing and managing `rl_tools` components (Actor/Critic networks, Optimizers, Replay Buffer, OffPolicyRunner) is largely placeholder. This includes memory allocation (`rlt::malloc`), initialization, and deallocation (`rlt::free`).
    *   **Impact:** The agent cannot be correctly set up for training or inference.
    *   **Recommendation:** Complete the C++ implementation for these core `rl_tools` object lifecycles in `URLAgentManager.cpp`.

## Phase 3: Blueprint Exposure Layer

1.  **Item 3.1: Blueprint Function Libraries**
    *   **Issue:** No `UBlueprintFunctionLibrary` classes for RL-specific static utility functions have been implemented.
    *   **Recommendation:** Implement if deemed necessary for better Blueprint workflow organization.

2.  **Item 3.3.2: `URLAgentManager` Blueprint-Callable Functions - C++ Implementation Details**
    *   **Issue:** Several core C++ implementations for Blueprint-exposed functions in `URLAgentManager.cpp` are placeholders:
        *   `GetAction()`: Returns random actions instead of using a policy.
        *   `LoadPolicy()` / `SavePolicy()`: Do not implement actual file I/O with `rl_tools` policy formats.
        *   `PerformTrainingStep()`, `CollectExperience()`, `UpdateNetworks()`: Lack the actual `rl_tools` calls for experience collection, replay, and network updates.
    *   **Impact:** Training and inference functionalities are not operational.
    *   **Recommendation:** Complete the C++ implementations for these functions using appropriate `rl_tools` operations.

3.  **Item 3.4.1: Asynchronous Training Operations**
    *   **Issue:** The training process initiated by `StartTraining()` and managed by `StepTraining()` appears to be synchronous. Long training sessions driven from a single Blueprint call could block the game thread.
    *   **Recommendation:** Refactor the training loop in `URLAgentManager` to run on a separate thread or use an asynchronous task manager (`FAsyncTask`, `FTSTicker`) to prevent game thread stalls. Provide appropriate Blueprint events for progress and completion.

4.  **Item 3.5.2: Observation/Action Normalization/Denormalization**
    *   **Issue:** There is no visible support for configuring or applying observation/action normalization or denormalization, which is often crucial for stable RL training.
    *   **Recommendation:** Add configurable options (e.g., in `FRLEnvironmentConfig` or `FRLTrainingConfig`) and implement the normalization/denormalization logic within the data flow between UE and `rl_tools`.
